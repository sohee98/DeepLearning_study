# 파이토치

## 1. 텐서 Tensor

* 1차원=벡터, 2차원=행렬, 3차원=텐서

* |t|=(batch size, width, height)

* 배열이나 행렬과 매우 유사한 특수한 자료구조

* 텐서를 사용하여 모델의 입력과 출력, 그리고 모델의 매개변수들을 부호화함

* ```python
  import torch
  import numpy ad np
  ```

* 파이토치 텐서의 특징

  * Numpy-like: Storage efficient, Matrix operations
  * Support GPU computing
  * Keep track of graph of computations 각 텐서들이 어떻게 계산되었는지 가지고 있음.

#### 텐서 초기화

```python
data = [[1, 2],[3, 4]]
x_data = torch.tensor(data)		# 직접 생성
# t = torch.FloatTensor([0., 1., 2., 3.])

np_array = np.array(data)		# 2차원 행렬 만듬
x_np = torch.from_numpy(np_array)		# numpy 배열로부터 생성
```

* 무작위, 상수값 사용

  * ```python
    shape = (2,3,)
    rand_tensor = torch.rand(shape)
    ones_tensor = torch.ones(shape)
    zeros_tensor = torch.zeros(shape)
    ```

  * ```
    Random Tensor:
     tensor([[0.7759, 0.1138, 0.4539],
            [0.6169, 0.2986, 0.2102]])
    
    Ones Tensor:
     tensor([[1., 1., 1.],
            [1., 1., 1.]])
    
    Zeros Tensor:
     tensor([[0., 0., 0.],
            [0., 0., 0.]])
    ```
  
  * ```python
    x = torch.FloatTensor([[0, 1, 2], [2, 1, 0]])
    print(torch.ones_like(x)) # 입력 텐서와 크기를 동일하게 하면서 값을 1로 채우기
    print(torch.zeros_like(x)) # 입력 텐서와 크기를 동일하게 하면서 값을 0으로 채우기
    ```
  
  * ```
    tensor([[1., 1., 1.], [1., 1., 1.]])
    tensor([[0., 0., 0.], [0., 0., 0.]])
    ```

#### 텐서의 속성 (Attribute)

> tensor = torch.rand(3,4)

* 모양 : `tensor.size()` / `tensor.shape` => torch.Size([3,4])

* 자료형 : `tensor.dtype` => torch.float32
* 저장된 장치 : `tensor.device` => cpu

#### 텐서 연산 (Operation)

> tensor = torch.ones(4, 4)

* 인덱싱

  * 첫번째 행 : tensor[0]

  * 첫번째 열 : tensor[: ,0]

  * 마지막 열 : tensor[..., -1]

* 슬라이싱

  * ```python
    tensor[:,1] = 0
    ```

  * ```
    tensor([[1., 0., 1., 1.],
            [1., 0., 1., 1.],
            [1., 0., 1., 1.],
            [1., 0., 1., 1.]])
    ```

* 텐서 합치기 : `.cat()`

  * ```python
    t1 = torch.cat([tensor, tensor, tensor], dim=1)
    ```

    > dim=0 / dim=1 : 첫번째/두번째 차원을 늘리라는 의미
    > dim=0 : (4,4) => (12,4)

  * ```
    tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],
            [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],
            [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],
            [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])
    ```

* 스택킹 : `.stack()`

  * ```python
    x = torch.FloatTensor([1, 4])	# (2,) 크기 3개
    y = torch.FloatTensor([2, 5])
    z = torch.FloatTensor([3, 6])
    print(torch.stack([x, y, z]))	# (3, 2) 
    ```

  * ```
    tensor([[1., 4.],
            [2., 5.],
            [3., 6.]])
    ```

  * dim 인자 추가 가능

    * ```python
      print(torch.stack([x, y, z], dim=1))	# (2, 3)
      ```

    * ```
      tensor([[1., 2., 3.],
              [4., 5., 6.]])
      ```

* 산술 연산

  * 곱셈

    * 두 텐서 간의 행렬 곱 : `@`, `.matmul()`

        * ```python
          y1 = tensor @ tensor.T
          y2 = tensor.matmul(tensor.T)
          
          y3 = torch.rand_like(tensor)
          torch.matmul(tensor, tensor.T, out=y3)
          ```

    * 요소별 곱 : `*`, `.mul()`

        * ```python
          z1 = tensor * tensor
          z2 = tensor.mul(tensor)
          
          z3 = torch.rand_like(tensor)
          torch.mul(tensor, tensor, out=z3)
          ```
    
  * 평균 : `.mean()`

    * ```python
      t = torch.FloatTensor([[1, 2], [3, 4]])
      print(t.mean(dim=0))
      ```

    * 인자로 dim을 주면 해당 차원을 삭제

      * `dim=0` : 행렬의 첫번째 차원 = 행

        * (2,2) => (1,2) = (,2) = 열의 차원만 보존

        * ```python
          [[1., 2.],
           [3., 4.]]
          
          1과 3의 평균을 구하고, 2와 4의 평균을 구한다.
          결과 ==> [2., 3.]
          ```

      * `dim=1` , `dim=-1`: 행렬의 두번째/마지막 차원 = 열 

        * ```python
          print(t.mean(dim=-1))
          tensor([1.5000, 3.5000])
          ```

  * 덧셈 : `.sum()`

  * 최대(Max)와 아그맥스(ArgMax)

    * ```python
      print(t.max()) # Returns one value: max
      tensor(4.)
      ```

    * ```python
      print(t.max(dim=0)) # Returns two values: max and argmax
      (tensor([3., 4.]), tensor([1, 1]))
      ```

      dim 인자를 주게되면 max와 argmax 함께 리턴한다.

    * argmax : 인덱스

* 단일 요소 텐서

  * 요소가 하나인 텐서의 경우, `item()`을 사용하여 파이썬 숫자 값으로 변환할 수 있다.

  * ```python
    agg = tensor.sum()
    agg_item = agg.item()
    ```

* 바꿔치기 연산

  * 연산결과를 피연산자에 저장하는 연산. `_` 접미사

  * ```python
    tensor.add_(5)		# 5더해진 값이 tensor에 저장됨
    ```

* 텐서 transport

  * ```python
    x = torch.zeros(5,3)
    x_t = x.t()		# 3 * 5 로 변환. 하지만 storage 같은 storage 공간에 저장되어있음
    ```

#### NumPy 변환 (Bridge)

* 텐서 => NumPy 배열로 변환

  * CPU상의 텐서와 NumPy 배열은 메모리 공간을 공유하기 떄문에, 하나를 변경하면 다른 하나도 변경됩니다.

  * ```python
    t = torch.ones(5)
    print(f"t: {t}")
    n = t.numpy()
    print(f"n: {n}")
    ```

  * ```
    t: tensor([1., 1., 1., 1., 1.])
    n: [1. 1. 1. 1. 1.]
    ```

  * 텐서의 변경 사항이 NumPy 배열에 반영됨

  * ```python
    t.add_(1)
    ```

  * ```
    t: tensor([2., 2., 2., 2., 2.])
    n: [2. 2. 2. 2. 2.]
    ```

* NumPy 배열 => 텐서로 변환

  * ```python
    n = np.ones(5)
    t = torch.from_numpy(n)
    ```

  * NumPy 배열의 변경사항이 텐서에 반영됨

#### 텐서 조작하기 (Manipulation)

* 뷰 (View) : 원소의 수를 유지하면서 텐서의 크기 변경 `.view()`

  * view는 기본적으로 변경 전과 변경 후의 텐서 안의 원소의 개수가 유지되어야 한다

  * 파이토치의 view는 사이즈가 -1로 설정되면 다른 차원으로부터 해당 값을 유추한다

    * ```python
      x = torch.rand(8,8)
      x.view(64)
      x.view(4,16)  # 4 * 16 행렬로 표시
      x.view(-1,16)  # -1 = 자동값
      x.view(-1, 4, 4)
      ```

* 스퀴즈 (Squeeze) : 차원이 1인 경우 해당 차원을 제거 `.squeeze()`

  * ```python
    ft = torch.FloatTensor([[0], [1], [2]])
    print(ft.squeeze())
    print(ft.squeeze().shape)
    ```

  * ```
    tensor([0., 1., 2.])
    torch.Size([3])
    ```

* 언스퀴즈 (Unsqueeze) : 특정 위치에 1인 차원 추가 `.unsqueeze(0)`

  * ```python
    ft = torch.Tensor([0, 1, 2])
    print(ft.unsqueeze(0)) # 인덱스가 0부터 시작하므로 0은 첫번째 차원을 의미한다.
    print(ft.unsqueeze(0).shape)
    ```

  * ```
    tensor([[0., 1., 2.]])
    torch.Size([1, 3])
    ```

  * (3,) 크기의 1차원 벡터가 (1,3) 크기의 2차원 텐서로 변경

  * view로도 구현 가능

    * ```
      print(ft.view(1, -1))
      print(ft.view(1, -1).shape)
      ```

* 타입 캐스팅 (Type Casting)
  * ![img](https://wikidocs.net/images/page/52846/newimage.png)



## 2. 역전파

> 자동 미분 기능

1. 변수 선언 (+데이터 입력)

   ```python
   x = torch.ones(2,2, requires_grad=True) # x에관한 연산을 추적할 수 있게 함
   print(x)
   ```

2. 모델 내 연산 예측값 산출

   ```python
   y = x + 1
   print(y)
   ```

3. 손실함수 계산

   ```python
   z = 2*y**2
   res = z.mean()
   ```

4. 손실 산출

   ```python
   print(res)
   ```

```python
# d(res)/dx_i = x_i + 1
# res = (z_1 + .. z_4)/4
# z_i = 2 y_i ** 2
# z_i = 2(x_i+1) ** 2 
res.backward()		# gradient 계산하기
print(x.grad)
```

> tensor([[2., 2.],  [2., 2.]])

> ```python
> import torch.nn as nn
> import torch.nn.functional as F
> ```



## 3. 데이터 불러오기 + 커스터마이징

> 1. 파이토치 제공 데이터 사용
> 2. 같은 클래스 별 푤더 이미지 데이터 이용
> 3. 개인 데이터 사용 (2types)

```python
import torch
import torchvision
import torchvision.transforms as tr
from torch.utils.data import DataLoader, Dataset
import numpy as np
```

1. 파이토치 제공 데이터 사용

   ```python
   transf = tr.Compose([tr.Resize(8), tr.ToTensor()])	# 전처리 작업
   # Transforms on PIL Image
   # Pad, Grayscale, RandomCrop, Normalize, ..
   # Trancsforms on torch.*Tensor = tensor image
   # torchvision.transforms.ToPILImage(mode=None) ...
   # ...
   ```

   ```python
   trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transf)
   testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transf)
   ```

   ```
   trainset[0][0].size()
   ```

   > torch.Size([3, 8, 8])
   > => 채널3개에 8 by 8 이미지

   ```
   trainloader = DataLoader(trainset, batch_size=50, shuffle=True, num_workers=2)
   testloader = DataLoader(testset, batch_size=50, shuffle=True, num_workers=2)
   ```

   ```
   len(trainloader)
   ```

   > 1000  => batch size 50 짜리가 1000개 있다 (trainset 총 50000개임)

   * transforms, torchvision.datasets => DataLoader 3줄이면 끝

   ```python
   dataiter = iter(trainloader)
   images, labels = dataiter.next()
   images.size()
   ```

   > torch.Size([50, 3, 8, 8]) => batch size 5개, 채널 3개, 이미지 사이즈 8*8

2. 같은 클래스 별 폴더 이미지 데이터 이용

   ```python
   # ./class/tiger ./class/lion
   transf = tr.Compose([tr.Resize(16),tr.ToTensor()])	# 전처리 작업
   trainset = torchvision.datasets.ImageFolder(root='./class', transform=transf)
   trainloader = DataLoader(trainset, batch_size=10, shuffle=False, num_workers=2)
   print(len(trainloader))	
   ```

   > 3 => 배치 덩어리 수

   ```python
   trainset[0][0].size()
   ```

   > torch.Size([3, 16, 16])  => 채널 3개

 3. 개인 데이터 사용 (2 types)

    ```python
    # import preprocessing
    # NumPy 형태로 들어왔다고 가정, 32*32이미지 3채널 20개 가정 => 레이블 20개 가정
    train_images = np.random.randint(256, size=(20, 32, 32, 3))
    train_labels = np.random.randint(2, size=(20, 1))
    
    # preprocessing.....
    # train_images, train_labels = preprocessing(train_images, train_labels)
    print(train_images.shape, train_labels.shape
    ```

    > (20, 32, 32, 3) (20, 1)

    ```python
    class TensorData(Dataset):
        def __init__(self, x_data, y_data):
            self.x_data = torch.FloatTensor(x_data)
            self.x_data = self.s_data.permute(0,3,1,2) #이미지개수,채널수,이미지너비높이
            self.y_data = torch.LongTensor(y_data)
            self.len = self.y_data.shape[0]
            
        def __getitem__(self, index):
            return self.x_data[index], self.y_data[index]
        
        def __len__(self):
            return self.len
    ```

    ```python
    train_data = TensorData(train_images, train_labels)
    train_loader = DataLoader(train_data, batch_size=10, shuffle=True)
    train_data[0][0].size()
    ```

    > torch.Size([3, 32, 32])

    ```python
    dataiter = iter(train_loader)
    images, labels = dataiter.next()
    images.size()
    ```

    > torch.Size([10, 3, 32, 32])

    * 폴더 정리를 못하는 경우
      * 다른 작업과 공용으로 사용
      * 폴더가 아닌 SQL 같은 곳에서 넘어오는 경우

    


