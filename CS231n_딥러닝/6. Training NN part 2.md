# [6강] Training NN part 2

### - Parameter update schemes

* Momentum update

  <img src="md-images/image-20220320185925202.png" alt="image-20220320185925202" style="zoom:50%;" />

  > mu : 마찰계수 (설정됨)

  <img src="md-images/image-20220320190013234.png" alt="image-20220320190013234" style="zoom:50%;" />

* Nesterov Momentum update

  <img src="md-images/image-20220320190054722.png" alt="image-20220320190054722" style="zoom: 50%;" />



<img src="md-images/image-20220320190233781.png" alt="image-20220320190233781" style="zoom: 67%;" />

* 많은 방법들 = First order optimization methods (gradient만 사용)
* 요즘은 Adam



### - Dropout - regularization

* randomly set some neurons to zero in the forward pass
  일부를 0으로 설정
* At training time ...

<img src="md-images/image-20220320190748409.png" alt="image-20220320190748409" style="zoom:50%;" />

* why good?
  * Forces the network to have a redundant representation
    중복을 가짐
  * training a large ensemble of models
* At test time ...
  * dropout 사용하지 않음

<img src="md-images/image-20220320191040763.png" alt="image-20220320191040763" style="zoom:50%;" />

<img src="md-images/image-20220320191100118.png" alt="image-20220320191100118" style="zoom:50%;" />





