# [13강] Segmentation, Attention

<img src="md-images/image-20220321154518742.png" alt="image-20220321154518742" style="zoom: 50%;" />

## 1. Segmentation

### Semantic Segmentation

<img src="md-images/image-20220321154544843.png" alt="image-20220321154544843" style="zoom:67%;" />

* 특징
  * classification : one label per image
  * semantic segmentation : one label per pixel 
  * 모든 픽셀을 라벨링 한다.
  * instance를 인식하지 못한다.

* 기본 파이프라인
  * <img src="md-images/image-20220321154852174.png" alt="image-20220321154852174" style="zoom:50%;" /> : 비용이 많이 듬
  * <img src="md-images/image-20220321154941949.png" alt="image-20220321154941949" style="zoom: 50%;" /> : 이미지 자체를 CNN에 넣어줌
    * 모든 픽셀에 관한 결과를 한번에 구해냄
    * down sampling이 발생함 => out image가 작아짐

* 확장된 버전들 

  * Multi-Scale : 다양한 스케일로 resize => 각각 CNN => up-sampling

    * <img src="md-images/image-20220321155248551.png" alt="image-20220321155248551" style="zoom:50%;" />

  * Refinement 

    * input 이미지를 3개의 채널(RGB) 분리해서 CNN 적용=> 이미지에 있는 모든 label 얻음
    * => 결과 : 원본보다 down sampling 됨
    * => 다시한번 CNN 적용 => 다시 refinement => 반복!
    * <img src="md-images/image-20220321155531654.png" alt="image-20220321155531654" style="zoom:50%;" />

  * Upsampling

    * 차이점 : 작아지는 feature map을 복원 = upsampling

    * upsampling 작업 까지도 네트워크의 일부분으로 편입. 

    * 마지막 layer = 학습이 가능한 learnable upsampling

    * <img src="md-images/image-20220321155717911.png" alt="image-20220321155717911" style="zoom:50%;" />

    * <img src="md-images/image-20220321160014930.png" alt="image-20220321160014930" style="zoom:50%;" />

    * skip connection : pool 5보다 초기의 pooling 단계에서 또다른 convolutional feature map 추출 => pool 4 => ...

    * Learnable Upsampling : "Deconvolution" (이름 부적절하다.)

      <img src="md-images/image-20220321160244845.png" alt="image-20220321160244845" style="zoom:50%;" />



### Instance Segmentation

<img src="md-images/image-20220321154709091.png" alt="image-20220321154709091" style="zoom:67%;" />

* instance 간에 구분함 (Semantic은 instance구분 못함)
* 비교적 최근 방식
* R-CNN과 비슷, but with segments

<img src="md-images/image-20220321160444025.png" alt="image-20220321160444025" style="zoom: 67%;" />

* 확장된 버전들

  * Hypercolumns

     <img src="md-images/image-20220321160509743.png" alt="image-20220321160509743" style="zoom: 67%;" />

  * Cascades

     <img src="md-images/image-20220321160606304.png" alt="image-20220321160606304" style="zoom:67%;" />



* 정리

<img src="md-images/image-20220321160644932.png" alt="image-20220321160644932" style="zoom:80%;" />



## 2. Attention

>  <img src="md-images/image-20220321160807007.png" alt="image-20220321160807007" style="zoom:50%;" />

<img src="md-images/image-20220321161008743.png" alt="image-20220321161008743" style="zoom:55%;" />

* 연산을 통해 summarize vector를 생성하는 방법

  * Soft vs Hard Attention

    <img src="md-images/image-20220321161146942.png" alt="image-20220321161146942" style="zoom:60%;" />

    <img src="md-images/image-20220321161228193.png" alt="image-20220321161228193" style="zoom:67%;" />



* Soft Attention for Translation

  <img src="md-images/image-20220321161527197.png" alt="image-20220321161527197" style="zoom:67%;" />

* Soft Attention for Everything!

  <img src="md-images/image-20220321161456293.png" alt="image-20220321161456293" style="zoom:67%;" />

* Attending to Arbitrary Regions

  * Attention의 한계점 : fixed grid에 한정되어 attention을 주게됨
  * 임의의 지점에 attention을 줌 : DRAW, Spatial Transformer Network 모델
  * <img src="md-images/image-20220321161904689.png" alt="image-20220321161904689" style="zoom:67%;" />

  * Spatial Transformer Networks
    * <img src="md-images/image-20220321162110753.png" alt="image-20220321162110753" style="zoom:60%;" />
    * <img src="md-images/image-20220321162133928.png" alt="image-20220321162133928" style="zoom: 60%;" />



* 정리

<img src="md-images/image-20220321162200778.png" alt="image-20220321162200778" style="zoom:80%;" />



















